---
title: "BeeImpactEvidence"
output: html_notebook
---

# Mapping the evidence for impacts of managed bees on wild bees

## Aims: 
Mapping evidence for the impacts of managed bees on wild bees, and propose a standardisation of terms and traits to measure / reporting.

##Methods: 

1.	*Development of search terms and desired inclusion/exclusion criteria*
    a.	discussion
    b.	litsearchr
2.	*Development of database*
    a.	WoS search
    b.	scimeetr /scholarsreadinglist
    c.	synthesisr
    d.  basic cleaning 
3.	*Mapping of evidence into categories*
    a.	scimeetr /scholarsreadinglist: groups, keyword lists, key papers, review papers
    b.	(EviAtlas)
    c.	(revtools)
4.	*Select categories of interest*
    a.  3 categories: exclude, include, include-focus for further data extraction
5.	*Extract the key metadata (automated)*
    a.	doi2text
    b.	Metadata: (GBIF sampling event style, darwincore)
    
Metadata (to be discussed): 
-Study location(s) region, country, site (methods includes 'study site')
-Study site characteristics ?? 
-Sampling method ?? search for key terms in methods
-Sampling size/duration ?? search for time delimiters
-Species groups, species ?? search for key delimiters?
-Variables ?? search for key variables
-Study purpose (aims - last paragraph of introduction)
-Authors/institutions (from existing metadata)

# install required packages
```{r install packages, eval = FALSE}
# remotes::install_github("MaximeRivest/scimeetr")
# remotes::install_github("elizagrames/litsearchr")
# remotes::install_github("rmetaverse/eviatlas")
# install.packages("synthesisr")
# install.packages("revtools")
```

```{r load packages, warning = FALSE, message = FALSE}
library(tidyverse) # programming
library(synthesisr) # db management 
library(scimeetr) # bibiometric analysis and determination of sub-communities
#library(eviatlas) # mapping of corpus (requires more building of df elsewhere)
#library(revtools) # mapping of corpus
```

# Development of search terms
This used litsearchr (other doc has details)

Search terms are in 3 groups: Managed bee, Native bee, and Interaction terms:

*Managed*: (African* NEAR bee) OR Apis OR Bombus OR "bumble bee" OR bumblebee* OR "honey bee" OR honeybee OR ((introduc* OR inva* OR non-native OR nonnative OR commercial OR exotic OR feral OR managed) NEAR (bee OR pollin*))
*Native*: (((cavity OR ground) NEAR nesting) OR (native OR solitary OR wild)) NEAR (bee OR pollin*)
*Interaction*: pollinat* OR network* OR “niche overlap” OR “partitioning” OR interact* OR competit* OR facilitat* OR mutualis* OR “resource limitation” OR hybridization OR introgression OR dependence OR assemblag* OR overlap OR spillover OR impact*

Search Web of Science: TOPIC searches Title, Abstract, Keywords (supplied by the author), and Keywords Plus (algorithm extraction of expanded terms stemming from the cited references)

TOPIC: ((African*  NEAR bee)  OR Apis  OR Bombus  OR "bumble bee"  OR bumblebee*  OR "honey bee"  OR honeybee  OR ((introduc*  OR inva*  OR non-native  OR nonnative  OR commercial  OR exotic  OR feral  OR managed)  NEAR (bee  OR pollin* ))) AND TOPIC: ((((cavity  OR ground)  NEAR nesting)  OR (native  OR solitary  OR wild))  NEAR (bee  OR pollin* )) AND TOPIC: (pollinat*  OR network*  OR “niche overlap”  OR “partitioning”  OR interact*  OR competit*  OR facilitat*  OR mutualis*  OR “resource limitation”  OR hybridization  OR introgression  OR dependence  OR assemblag*  OR overlap  OR spillover  OR impact*)

Timespan: All years. Indexes: SCI-EXPANDED, SSCI, A&HCI, ESCI.

Search date: 05.Nov.2020
Results: 2,400

Export full record and cited references in Tab-delimited (Win, UTF-8) format (for scimeetr), and in bib format (for revtools)

# synthesisr/revtools
compiles the retrieved objects (note - this is used further below, but put here in case multiple lists are being compiled)
and write to one file
```{r synthesisr compile}

dd <- "./data/raw_WoS_20201105/as_bib/"
synthesisr_df <- read_refs(list.files(dd, full.names = TRUE), return_df = TRUE, verbose = TRUE)
write_csv(synthesisr_df, "./data/raw_WoS_20201105/as_df/synthesisr_df.csv")
write_bibliography(synthesisr_df, "./data/raw_WoS_20201105/as_df/synthesisr_df.bib", format = "bib")

sum(!is.na(synthesisr_df$doi)) #2122 / 2249

fieldtags <- read_csv("wos_fieldtags.csv")
fieldtags %>% filter(TAG == "ID")

sdf <- summarise_all(synthesisr_df, function(x) sum(!is.na(x))) %>% 
    t() %>% 
    as_tibble(rownames = "TAG") %>% 
    select(TAG, "is_not_NA" = V1) 
sdf %>% print(n = nrow(sdf))
```

# EviAtlas
not so useful yet...need to build the df more
```{r open shily eviatlas, eval = FALSE}
# eviatlas(max_file_size = 200)
```

# custom plotting
Of any variables in the data frame. If the variables include multi-categories (e.g. multiple keywords, multiple research areas) these will be decomposed into their single elements before the frequency table is calculated.

```{r define plot_freq}

#' plot_freq
#' 
#' Plot variables from the review metadata. If the variables include multi-categories 
#' (e.g. multiple keywords, multiple research areas) these will be decomposed into their 
#' single elements before the frequency table is calculated.
#'
#' @param df A data table object describing metadata variables (columns) for each of the row article entries (rows)
#' @param var Length-1 character sting vector of the variable to be plotted
#' @param sort One of 'abc' (sort increasing alphabetical, default), 'cba' (sort decreasing alphabetical), 
#'    '123' (sort increasing by frequency), or '321' (sort decreasing by frequency)
#' @param nlim Length-1 numeric. Threshold frequency for including in plot. Defaults to 0 (all included).
#' @param sep Length-1 character string. Denotes the seperation characters to split multi-category variables.
#'
#' @return A ggplot object
#' @export
#'
#' @examples
plot_freq <- function(df, var, sort = 'abc', nlim = 0, sep = "; "){
    fdf <- df %>% 
        pull(var) %>% 
        str_split(pattern = sep) %>% 
        unlist() %>% 
        table(dnn = "var") %>% 
        as_tibble() %>%   
        filter(n >= nlim)
    
    xlev <- switch(sort,
                    'abc' = fdf %>% arrange(var) %>% pull(var),
                    'cba' = fdf %>% arrange(desc(var)) %>% pull(var),
                    '123' = fdf %>% arrange(n) %>% pull(var),
                    '321' = fdf %>% arrange(desc(n)) %>% pull(var),
                   fdf %>% pull(var)
                   )
    
    fdf <- fdf %>% 
        mutate_at("var", factor, levels = xlev)

    ggplot(fdf) +
        geom_col(aes(x = var, y = n)) +
        labs(x = var, y = "Frequency") +
        theme_bw() + 
        theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
}

```

```{r apply plot_freq, fig.width = 4, fig.height = 3}
plot_freq(synthesisr_df, "year") 
plot_freq(synthesisr_df, "research_areas", sort = '321', nlim = 100) 
plot_freq(synthesisr_df, "web_of_science_categories", sort = '321', nlim = 100) 
```

# scimeetr 
This requires full-record tab delimited files (can take multiple as a folder).
Summary characterises the entire corpus, but we will split it into communities before looking too deeply.
```{r scimeetr compile}
#library(scimeetr)

dd <- "./data/raw_WoS_20201105/as_txt/"
scimeetr_list <- import_wos_files(dd)
scimeetr_list
summary(scimeetr_list)

```

Scimeetr objects are a *list* of communities.
    Each community is a *list* of 8 items:
        $dfsci, the 'data.frame':	here, 2249 obs. of  68 variables (i.e. the fields imported directly from wos, see below)
        $tag the 6 tags that define the community
        $kw the keyword frequency table (combines both keywords-plus and author-keywords)
        $de the author-keyword frequency table
        $ti the title-word frequency table
        $ab the abstact word frequency table
        $cr the cited reference frequency (i.e. references cited by the corpus)
        $au the author frequency
 
In the dfsci, the 68 variables imported from WoS and their frequency of non NA data:      
```{r scimeetr available data}
indf <- summarise_all(scimeetr_list$com1$dfsci, function(x) sum(!is.na(x))) %>% 
    t() %>% 
    as_tibble(rownames = "TAG") %>% 
    select(TAG, "is_not_NA" = V1) %>% 
    right_join(., fieldtags) %>% 
    filter(is_not_NA > 0)

indf %>% print(n = nrow(indf))

```
        
# Use scimeetr to define sub-communities 
Options for coupling by: 'bic' for bibliographic coupling, 'kec' for keyword coupling, 'tic' for title coupling, 'abc' for abstract coupling, 'joc' for journal coupling, 'auc' for author coupling', 'woc' for word coupling, 'bickec' for a combination of bibliographic coupling and keyword coupling, and 'bickecticjoc' for a combination of bic, kec, tic, joc (this might be the better way for most use but it is a bit slow, this is why it is not the default choice).

Can iteratively apply, to create nested sub-communities - we'll do this twice to start, using the recommended bickecticjoc method.
```{r define sub-communities}
sci_bktj30 <- scimap(scimeetr_list, coupling_by = 'bickecticjoc', community_algorithm = 'louvain', min_com_size = 30)
scisub_bktj30 <- scimap(sci_bktj30, coupling_by = 'bickecticjoc', community_algorithm = 'louvain', min_com_size = 30)

```

This clustering uses igraph, creating a graph object (using igraph::graph_from_data_frame), weighting by assiciation between the (coupling_by) keywords, title, abstract, and (split) cited references

clusterised (clusterize) using the specified community algorithm (e.g. 'louvain', 'fast greedy') (wrappers for igraph::cluster_louvain(), igraph::cluster_fast_greedy()). igraph::cluster_louvain() implements the multi-level modularity optimization algorithm for finding community structure, see VD Blondel, J-L Guillaume, R Lambiotte and E Lefebvre: Fast unfolding of community hierarchies in large networks, http://arxiv.org/abs/arXiv:0803.0476 for the details.It is based on the modularity measure and a hierarchial approach. Initially, each vertex is assigned to a community on its own. In every step, vertices are re-assigned to communities in a local, greedy way: each vertex is moved to the community with which it achieves the highest contribution to modularity. When no vertices can be reassigned, each community is considered a vertex on its own, and the process starts again with the merged communities. The process stops when there is only a single vertex left or when the modularity cannot be increased any more in a step. Alternatively, igraph::cluster_fast_greedy() implements the fast greedy modularity optimization algorithm for finding community structure, see A Clauset, MEJ Newman, C Moore: Finding community structure in very large networks, http://www.arxiv.org/abs/cond-mat/0408187 for the details.

Note, includes some data cleaning (using tm) of abstracts, titles, e.g. tolower, remove punctuation and numbers, 
meaningless_word <- c(tm::stopwords("english"), 'use', 'used', 'using', 'uses',
                          'new', 'effect', 'effects', 'affect', 'affects', 'impact',
                          'impacts', 'implication', 'implications', 'potential',
                          'influence', 'influences', 'influenced', 'study', '-',
                          'data', 'can', 'results', 'different', 'similar', 'also',
                          'c', 'may', 'based', 'important', 'within','however',
                          'found', 'analysis', 'changes', 'among', 'large',
                          'number', 'higher', 'well', 'studies', 'total',
                          'increased', 'increases', 'elsevier', 'level', 'many',
                          'rights', 'present', 'will', 'low', 'across', 'showed',
                          'associated', 'approach', 'related', 'provide', 'including',
                          'increase')

Because this takes a moment, we can save this to a file for reloading after
```{r save and reload subcommunities, eval = FALSE}
saveRDS(scisub_bktj30, "data/raw_WoS_20201105/scisub_bktj30.RDS")
scisub_bktj30 <- readRDS("data/raw_WoS_20201105/scisub_bktj30.RDS")

```

Now we can look into the communities - particularly in the table of the discriminant keywords (tags) at the end:
```{r summary sub-communities}
summary(scisub_bktj30)

# plotting doesnt work too well, will need improving
plot(summary(scisub_bktj30, com_size = 30))

```
Plotting the scimeetr object uses  igraph::plot.igraph

The overall corpus seems relevant in terms of keywords and journals. 

These split the data into:

* Ecosystem services, biodiversity, conservation (n = 750)
    - Urbanization, biodiversity and conservation (323)
    - Crop pollination ecosystem servies in agriculture (308)
    - Pollination, diversity and grazing (119)
* Foraging and pesticides (937)
    - Pollination and social & soliatry nesting biology (195)
    - Foraging and pollination (260)
    - genetic diversity and introgression (236)
    - Pesticides, pathogen spillover, risk assessment (246)
* Plant-interactions (712)
    - Pollination effectiveness, floral evolution/morphology, and fig wasps (170)
    - Breeding system, pollination efficiency, conservation (131)
    - Invasive species, competition, facilitation, plant-pollinator interactions (273)
    - Pollination and fruit set (138)

However, we likely want to go a little deeper to really understand what these groupings are, and which ones we would like to look at in more detail, or reduce further. 

We can characterise a corpus using:

* keywords with characterize_kw()
* title-words with characterize_ti()
* astract-words with characterize_ab()
* journals with characterize_jo()
* authors with characterize_au()
* universities with characterize_un()
* countries with characterize_co()

By default these are ranked by the relevance (frequency and relative frequency combined weighted by lambda)

We can also characterise a corpus by extracting the top ranked papers (a 'reading list'). Several sets of options are available:

* core_papers: rank by most cited 
* core_yr: most cited per year (from yrs -3 to -10)
* core_residual: rank by largest divergence from expected (based on fitted trend by age)

* by_expert_LC: returns m papers by k highest ranked (local harmonised H-index) authors
* by_expert_TC: returns m papers by k highest ranked (total harmonised H-index) authors
* group_of_experts_TC: returns m papers by k highest ranked (local harmonised H-index) author groups
* group_of_experts_LC:returns m papers by k highest ranked (total harmonised H-index) author groups

* cite_most_others: ranked by citations (review and well researched)
* betweeness: interdisciplinary
* closeness: large and wide list of citations
* connectness: tend to have cited what most other studies cited.

other undocumented options: 'direct_cite_eigen', 'link_strength', 'page_rank', 'journal_dis', 'journal_unique_combn'

To drill down levels, can specify with "dive_to" or "focus_on" these appear to largely work the same now.
```{r examine sub-communities example}

# examples 
characterize_kw(scisub_bktj30) %>% .$com1_1 %>% head()  # by default these are ranked by de_relevance
focus_on(scisub_bktj30, grab = "com1_1") %>% scilist(k = 3, reading_list = "core_papers")
dive_to(scisub_bktj30, aim_at = "com1_1") %>% scilist(k = 3, reading_list = "core_papers")
scilist_all(scimeetr_list, length_list = 1)

```

Function to 'deep-dive' into a corpus

A few more details from scimeetr documentation regarding the reading lists:

"Using scilist with reading_list = _"by_expert_LC"_ we will get a list of recent
papers by one or a few experts in the community. For the option by_expert_LC,
authors are ranked based on their harmonic local H-index. The H-index is a
measure of an other productivity and impact. An author with an H-index of 10
means that he has published at least 10 papers with 10 or more citation each. A
local H-index means that only citations from other papers in the community are
counted. A harmonic local H-index means that authors do not get the full credit
for each citation their paper received. It is corrected depending on the authos
position in the authors list. First authors gets most of the credit, then the
last author gets the second most, and the authors gets credit as a proportion of
their position. Once the authors harmonic-local-H-index is found they are ranked
and the m most recent publication of the k most 'expert' authors are listed as a
reading list."

Using scilist with reading_list = _"core_paper"_ we will get a list of the most cited papers _by papers in the corpus_. Not all of these papers are themselves in the corpus so when I try and match the title there are NAs produced. Therefore, I've added in a "most cited paper in the corpus" to, based on the number of citations (direct from WoS metadata). For the residuals version of this, I've used the same gam function to model, but have used either the publication year, or for the more recent papers that are only early access, the early access year. Questionable whether we want to use the early access year for all as the default. 

```{r define deep_dive into sub-communities}

# This function modifies the print.scimeetr function to include more summaries of interest for us at this stage
# Gets the group tags and top kw keywords, title-words, abstract-words, journals, and authors 
# Gets the top ranked papers in the group, by citations (n = kr) and authors (kr/mr authors and kr papers per author) (several versions, see above)
# Gets the papers most likely to give a good review of the literature in the group
# This returns results from the upper level community in the scimeetr object only.

deep_dive <- function(object, kw = 10, kr = 10, mr = 3, ...){
  
  out <- list()
  
  out$Overview <- list(
    str_c("Number of papers: ", nrow(object[[1]]$dfsci)),
    str_c("Number of communities: ", length(object)),
    object %>% map_dbl(function(x) nrow(x$dfsci)) %>% .[order(names(.))]
  )
  
  out$Word_frequencies <- list(
    tags = object[[1]]$tag,
    keywords = data.frame(
      "key_words" = object[[1]]$kw$ID[1:kw],
      "key_words_de" = object[[1]]$de$ID[1:kw],
      "title_words" = object[[1]]$ti$ID[1:kw],
      "abstract_words" = object[[1]]$ab$ID[1:kw], 
      "authors" = object[[1]]$au$ID[1:kw],
      "cited_refs" = object[[1]]$cr$ID[1:kw], 
      stringsAsFactors = F)
  )

  # prepare function for ranking papers
  get_pubs <- function(rlist){
      .kr <- ifelse(rlist %in% "by_expert_LC", ceiling(kr/mr), kr)
      scilist(object, k = .kr, m = mr, reading_list = rlist) %>% 
          .[[1]] %>% 
          select(publication) %>% 
          add_column(type = rlist) 
  }
  
  title_tab <- object[[1]]$dfsci %>% 
    select(RECID, Title = TI) %>% 
    mutate(publication = map_chr(RECID, function(x) x %>%  
                                 stringr::str_replace_all(', DOI.*', '') %>% 
                                 stringr::str_replace_all('V(?=[0-9]{1,6})', '') %>% 
                                 stringr::str_replace_all('P(?=[0-9]{1,6})', ''))) %>% 
    select(-RECID)
  
  cited_tab <- object[[1]]$dfsci %>% 
      select(RECID, TimesCited = Z9, Title = TI, PY, EA) %>% 
      mutate(Year = pmap_dbl(list(PY, EA), function(py,ea) ifelse(!is.na(py), py, ea %>% str_extract('[0-9]{4}') %>% as.numeric()))) %>% 
      select(-PY, -EA)
  
  cited_tab <-  cited_tab %>% 
      add_column(residuals = mgcv::gam(TimesCited ~ Year, data = cited_tab, family = "poisson")$residuals)
  
  cited_most <- bind_rows(
    top_n(cited_tab, kr, TimesCited) %>% add_column("cited_most" = 1, "cited_resid" = 0),
    top_n(cited_tab, kr, TimesCited) %>% add_column("cited_most" = 0, "cited_resid" = 1)
  ) %>%
    group_by(RECID, Title, Year, TimesCited, residuals) %>% 
    summarise_at(vars(cited_most:cited_resid), sum) %>% 
    arrange(desc(residuals), desc(TimesCited))
  
  out$Articles_highcited_in <- list(
    description = str_c("Highest cited papers in the group"),
    publist = cited_most
  )
  
  out$Articles_highcited_by <- list(
    description = str_c("Highest cited papers by the group"),
    publist = map(c("core_papers", "core_residual"), get_pubs) %>% 
      bind_rows() %>% 
      with(., table(publication, type)) %>% 
      as_tibble() %>% 
      pivot_wider(names_from = "type", values_from ="n") %>% 
      arrange(desc(core_papers), desc(core_residual)) %>% 
      left_join(title_tab, by = "publication")
  )
  
  out$Articles_experts <- list(
    description = str_c("Highest cited papers by highly cited authors"),
    publist = map(c("by_expert_LC"), get_pubs) %>% 
      bind_rows() %>% 
      with(., table(publication, type)) %>% 
      as_tibble() %>% 
      pivot_wider(names_from = "type", values_from ="n") %>% 
      arrange(desc(by_expert_LC)) %>% 
      left_join(title_tab, by = "publication")
  )
  
  out$Aricles_review <- list(
    description = str_c("Papers likely to provide a good overview of the category"),
    publist = map(c("cite_most_others", "connectness"), get_pubs) %>% 
      bind_rows() %>% 
      with(., table(publication, type)) %>% 
      as_tibble() %>% 
      pivot_wider(names_from = "type", values_from ="n") %>% 
      arrange(desc(cite_most_others), desc(connectness)) %>% 
      left_join(title_tab, by = "publication")
  )
  
  return(out)
}


```

## Apply the deep dive
Map to the different communities
```{r}

comlist <- sort(names(scisub_bktj30))

deep_dive_results <- tibble(
  community = comlist,
  dd_results = map(comlist, function(x) deep_dive(dive_to(scisub_bktj30, aim_at = x)))
  )

#deep_dive_results$dd_results[[1]]

```

  